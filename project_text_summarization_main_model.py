# -*- coding: utf-8 -*-
"""Project_Text_Summarization_Main_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ndjq-oSW0FHyNaR-bt_TdpV_9x1cfvKV
"""

from google.colab import drive
drive.mount('/content/drive')

# Import libraries
import pandas as pd
import numpy as np
import tensorflow as tf
import time
import re
import pickle

"""### Loading Data"""

# news = pd.read_excel("data/news.xlsx")
news = pd.read_csv("/content/drive/MyDrive/news_data.csv")

news.head()

news.shape

document = news['text']
summary = news['headlines']

document[13], summary[13]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(document, summary, test_size=0.30, random_state=42)

## Train and validation split
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.30, random_state=42)

"""### Preprocessing"""

document=X_train
summary=y_train

# for decoder part
summary = summary.apply(lambda x: '<start> ' + x + ' <stop>')
summary.head()

"""#### Tokenizing the texts into integer tokens"""

filters = '!"#$%&()*+,-./:;=?@[\\]^_`{|}~\t\n'
oov_token = '<unk>'

document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)
summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)

document_tokenizer.fit_on_texts(document)
summary_tokenizer.fit_on_texts(summary)

inputs = document_tokenizer.texts_to_sequences(document)
targets = summary_tokenizer.texts_to_sequences(summary)

summary_tokenizer.texts_to_sequences(["Testing statement here"])

summary_tokenizer.sequences_to_texts([[2806,2823,2481]])

encoder_vocab_size = len(document_tokenizer.word_index) + 1
decoder_vocab_size = len(summary_tokenizer.word_index) + 1

# vocab_size
encoder_vocab_size, decoder_vocab_size

"""#### Obtaining insights on lengths for defining maxlen"""

document_lengths = pd.Series([len(x) for x in document])
summary_lengths = pd.Series([len(x) for x in summary])

document_lengths.describe()

summary_lengths.describe()

# taking values greater than  75th percentile

encoder_maxlen = 220
decoder_maxlen = 56

"""#### Padding/Truncating sequences for identical sequence lengths"""

inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')
targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')

"""### Creating dataset pipeline"""

inputs = tf.cast(inputs, dtype=tf.int32)
targets = tf.cast(targets, dtype=tf.int32)

BUFFER_SIZE = 31000
BATCH_SIZE = 52

dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

"""### Positional Encoding for defining location of words"""

def get_angles(position, i, d_model):
    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
    return position * angle_rates

def positional_encoding(position, d_model):
    angle_rads = get_angles(
        np.arange(position)[:, np.newaxis],
        np.arange(d_model)[np.newaxis, :],
        d_model
    )

    # sin to even indices 
    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])

    # cos to odd indices 
    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])

    pos_encoding = angle_rads[np.newaxis, ...]

    return tf.cast(pos_encoding, dtype=tf.float32)

"""### Masking

- Padding mask for masking "pad" sequences
- Lookahead mask for masking future words from contributing in prediction of current words in self attention
"""

def create_padding_mask(seq):
    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
    return seq[:, tf.newaxis, tf.newaxis, :]

def create_look_ahead_mask(size):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask

"""### Building the Model

#### Scaled Dot Product
"""

def scaled_dot_product_attention(q, k, v, mask):
    matmul_qk = tf.matmul(q, k, transpose_b=True)

    dk = tf.cast(tf.shape(k)[-1], tf.float32)
    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

    if mask is not None:
        scaled_attention_logits += (mask * -1e9)  

    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)

    output = tf.matmul(attention_weights, v)
    return output, attention_weights

"""#### Multi-Headed Attention"""

class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model

        assert d_model % self.num_heads == 0

        self.depth = d_model // self.num_heads

        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)

        self.dense = tf.keras.layers.Dense(d_model)
        
    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    def call(self, v, k, q, mask):
        batch_size = tf.shape(q)[0]

        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)

        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)

        scaled_attention, attention_weights = scaled_dot_product_attention(
            q, k, v, mask)

        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])

        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
        output = self.dense(concat_attention)
            
        return output, attention_weights

"""### Feed Forward Network"""

def point_wise_feed_forward_network(d_model, dff):
    return tf.keras.Sequential([
        tf.keras.layers.Dense(dff, activation='relu'),
        tf.keras.layers.Dense(d_model)
    ])

"""#### Fundamental Unit of Transformer encoder"""

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(EncoderLayer, self).__init__()

        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
    
    def call(self, x, training, mask):
        attn_output, _ = self.mha(x, x, x, mask)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)

        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)

        return out2

"""#### Fundamental Unit of Transformer decoder"""

class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(DecoderLayer, self).__init__()

        self.mha1 = MultiHeadAttention(d_model, num_heads)
        self.mha2 = MultiHeadAttention(d_model, num_heads)

        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
        self.dropout3 = tf.keras.layers.Dropout(rate)
    
    
    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(attn1 + x)

        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(attn2 + out1)

        ffn_output = self.ffn(out2)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(ffn_output + out2)

        return out3, attn_weights_block1, attn_weights_block2

"""#### Encoder consisting of multiple EncoderLayer(s)"""

class Encoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):
        super(Encoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)

        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]

        self.dropout = tf.keras.layers.Dropout(rate)
        
    def call(self, x, training, mask):
        seq_len = tf.shape(x)[1]

        x = self.embedding(x)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)
    
        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training, mask)
    
        return x

"""#### Decoder consisting of multiple DecoderLayer(s)"""

class Decoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):
        super(Decoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)

        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(rate)
    
    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        seq_len = tf.shape(x)[1]
        attention_weights = {}

        x = self.embedding(x)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)

            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1
            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2
    
        return x, attention_weights

"""#### Finally, the Transformer"""

class Transformer(tf.keras.Model):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):
        super(Transformer, self).__init__()

        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)

        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)

        self.final_layer = tf.keras.layers.Dense(target_vocab_size)
    
    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):
        enc_output = self.encoder(inp, training, enc_padding_mask)

        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)

        final_output = self.final_layer(dec_output)

        return final_output, attention_weights

"""### Training"""

# hyper-parameters
num_layers = 4
d_model = 128
dff = 512
num_heads = 8
EPOCHS = 22

"""#### Adam optimizer with custom learning rate scheduling"""

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000):
        super(CustomSchedule, self).__init__()

        self.d_model = d_model
        self.d_model = tf.cast(self.d_model, tf.float32)

        self.warmup_steps = warmup_steps
    
    def __call__(self, step):
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)

        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

"""#### Defining losses and other metrics """

learning_rate = CustomSchedule(d_model)

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')

def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask

    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)

train_loss = tf.keras.metrics.Mean(name='train_loss')
val_loss=tf.keras.metrics.Mean(name='val_loss')

"""#### Transformer"""

transformer = Transformer(
    num_layers, 
    d_model, 
    num_heads, 
    dff,
    encoder_vocab_size, 
    decoder_vocab_size, 
    pe_input=encoder_vocab_size, 
    pe_target=decoder_vocab_size,
)

"""#### Masks"""

def create_masks(inp, tar):
    enc_padding_mask = create_padding_mask(inp)
    dec_padding_mask = create_padding_mask(inp)

    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
    dec_target_padding_mask = create_padding_mask(tar)
    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)
  
    return enc_padding_mask, combined_mask, dec_padding_mask

"""#### Checkpoints to reload the last trained weights"""

checkpoint_path = "checkpoints"

ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)

ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)

if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    print ('Latest checkpoint restored!!')
ckpt_manager.save()

"""#### Training steps"""

@tf.function
def train_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]

    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)

    with tf.GradientTape() as tape:
        predictions, _ = transformer(
            inp, tar_inp, 
            True, 
            enc_padding_mask, 
            combined_mask, 
            dec_padding_mask
        )
        loss = loss_function(tar_real, predictions)

    gradients = tape.gradient(loss, transformer.trainable_variables)    
    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))

    train_loss(loss)

@tf.function
def val_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]

    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)

    with tf.GradientTape() as tape:
        predictions, _ = transformer(
            inp, tar_inp, 
            True, 
            enc_padding_mask, 
            combined_mask, 
            dec_padding_mask
        )
        loss = loss_function(tar_real, predictions)

    gradients = tape.gradient(loss, transformer.trainable_variables)    
    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))

    val_loss(loss)

# Here last 5 epochs are shown. They are starting from 1 as we are loading from saved checkpoints

import random
random.seed(42)
for epoch in range(EPOCHS):
    start = time.time()

    train_loss.reset_states()
    val_loss.reset_states()
    for (batch, (inp, tar)) in enumerate(dataset):
        train_step(inp, tar)
        val_step(inp,tar)
      
    if (epoch + 1) % 5 == 0:
        ckpt_save_path = ckpt_manager.save()
        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))
    
    print ('Epoch {} train_Loss {:.4f}'.format(epoch + 1, train_loss.result()))
    print ('Epoch {} val_Loss {:.4f}'.format(epoch + 1, val_loss.result()))
    print ('Time taken for 1 epoch: {} secs\n'.format(time.time() - start))

checkpoint_path = "checkpoints/ckpt-5"
ckpt = tf.train.Checkpoint(transformer=transformer)

ckpt_path = tf.train.latest_checkpoint(checkpoint_path)
ckpt.restore(ckpt_path)

"""### Inference

#### Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears
"""

def evaluate(input_document):
    input_document = document_tokenizer.texts_to_sequences([input_document])
    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')

    encoder_input = tf.expand_dims(input_document[0], 0)

    decoder_input = [summary_tokenizer.word_index["<start>"]]
    output = tf.expand_dims(decoder_input, 0)
    
    for i in range(decoder_maxlen):
        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)

        predictions, attention_weights = transformer(
            encoder_input, 
            output,
            False,
            enc_padding_mask,
            combined_mask,
            dec_padding_mask
        )

        predictions = predictions[: ,-1:, :]
        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

        if predicted_id == summary_tokenizer.word_index["<stop>"]:
            return tf.squeeze(output, axis=0), attention_weights

        output = tf.concat([output, predicted_id], axis=-1)

    return tf.squeeze(output, axis=0), attention_weights

def summarize(input_document):
    summarized = evaluate(input_document=input_document)[0].numpy()
    summarized = np.expand_dims(summarized[1:], 0)  # not printing <start> token
    return summary_tokenizer.sequences_to_texts(summarized)[0]  # only one document

"""Evaluation on validation set"""

k=pd.DataFrame(X_val)
#k['text'].head()
k.shape

# Taking 1000 entries as colab got crashed at higher values
# same has been done for the other parts also

b=[]
for i in range(1000):
  b.append(summarize(k.iloc[i, 0]))

d2=pd.DataFrame(y_val)

"""Summary example on validation set"""

print(k.iloc[1, 0]) #X_val

print(d2.iloc[1, 0]) #ground truth #y_val

b[1]  # predicted

"""Evaluation metric"""

# Average Blue score for validation set

j=0
from nltk.translate.bleu_score import sentence_bleu
for i in range(1000):
  reference = d2.iloc[i, 0]
  candidate = b[i]
  score = sentence_bleu(reference, candidate)
  j=j+score
print(j/1000)

# Implementing rouge function
from __future__ import division
from itertools import chain


def get_unigram_count(tokens):
    count_dict = dict()
    for t in tokens:
        if t in count_dict:
            count_dict[t] += 1
        else:
            count_dict[t] = 1

    return count_dict


class Rouge:
    beta = 1

    
    def my_lcs_grid(x, y):
        n = len(x)
        m = len(y)

        table = [[0 for i in range(m + 1)] for j in range(n + 1)]

        for j in range(m + 1):
            for i in range(n + 1):
                if i == 0 or j == 0:
                    cell = (0, 'e')
                elif x[i - 1] == y[j - 1]:
                    cell = (table[i - 1][j - 1][0] + 1, '\\')
                else:
                    over = table[i - 1][j][0]
                    left = table[i][j - 1][0]

                    if left < over:
                        cell = (over, '^')
                    else:
                        cell = (left, '<')

                table[i][j] = cell

        return table

    
    def my_lcs(x, y, mask_x):
        table = Rouge.my_lcs_grid(x, y)
        i = len(x)
        j = len(y)

        while i > 0 and j > 0:
            move = table[i][j][1]
            if move == '\\':
                mask_x[i - 1] = 1
                i -= 1
                j -= 1
            elif move == '^':
                i -= 1
            elif move == '<':
                j -= 1

        return mask_x

    
    def rouge_l(cand_sents, ref_sents):
        lcs_scores = 0.0
        cand_unigrams = get_unigram_count(chain(*cand_sents))
        ref_unigrams = get_unigram_count(chain(*ref_sents))
        for cand_sent in cand_sents:
            cand_token_mask = [0 for t in cand_sent]
            cand_len = len(cand_sent)
            for ref_sent in ref_sents:
              
                Rouge.my_lcs(cand_sent, ref_sent, cand_token_mask)

          
            cur_lcs_score = 0.0
            for i in range(cand_len):
                if cand_token_mask[i]:
                    token = cand_sent[i]
                    if cand_unigrams[token] > 0 and ref_unigrams[token] > 0:
                        cand_unigrams[token] -= 1
                        ref_unigrams[token] -= 1
                        cur_lcs_score += 1

                     

            lcs_scores += cur_lcs_score

        ref_words_count = sum(len(s) for s in ref_sents)
        
        cand_words_count = sum(len(s) for s in cand_sents)
       

        precision = lcs_scores / cand_words_count
        recall = lcs_scores / ref_words_count
        f_score = (1 + Rouge.beta ** 2) * precision * recall / (recall +
                                                                Rouge.beta ** 2 * precision + 1e-7) + 1e-6  # prevent underflow
        return precision, recall, f_score

   


if __name__ == '__main__':
    r = Rouge()

# Rouge-l score for validation set

r = Rouge()
m=[]
i=0
while i<1000:
  m.append(r.rouge_l(b[i], d2.iloc[i, 0]))
  i=i+1

# Average for 1000 samples
p=0
r=0
f=0
for i in range(1000):
  f=f+m[i][2]
  p=p+m[i][0]
  r=r+m[i][1]
p=p/1000
f=f/1000
r=r/1000
print("Average Precision is :"+str(p)+"\nAverageRecall is :"+str(r)+"\nAverage F Score is :"+str(f))

"""Evaluation on test set"""

d=pd.DataFrame(X_test)
#d['text'].head()
d.shape

# Taking 1000 test samples
a=[]
for i in range(1000):
  a.append(summarize(d.iloc[i, 0]))

d1=pd.DataFrame(y_test)

"""Example of summaries shown in report

A) Good summaries
"""

print(d.iloc[493, 0]) #X_test

print(d1.iloc[493, 0]) #ground truth #y_test

a[493]  # predicted

print(d.iloc[244, 0]) #X_test

print(d1.iloc[244, 0]) #ground truth #y_test

a[244]  # predicted

"""B) Bad summaries"""

print(d.iloc[495, 0]) #X_test

print(d1.iloc[495, 0]) #ground truth #y_test

a[495]  # predicted

print(d.iloc[400, 0]) #X_test

print(d1.iloc[400, 0]) #ground truth #y_test

a[400]  # predicted

"""Rouge-L score for good summaries"""

r = Rouge()
[precision, recall, f_score] = r.rouge_l(a[493], d1.iloc[493, 0])
print("Precision is :"+str(precision)+"\nRecall is :"+str(recall)+"\nF Score is :"+str(f_score))

r = Rouge()
[precision, recall, f_score] = r.rouge_l(a[244], d1.iloc[244, 0])
print("Precision is :"+str(precision)+"\nRecall is :"+str(recall)+"\nF Score is :"+str(f_score))

"""Rouge-L score for bad summaries"""

r = Rouge()
[precision, recall, f_score] = r.rouge_l(a[495], d1.iloc[495, 0])
print("Precision is :"+str(precision)+"\nRecall is :"+str(recall)+"\nF Score is :"+str(f_score))

r = Rouge()
[precision, recall, f_score] = r.rouge_l(a[400], d1.iloc[400, 0])
print("Precision is :"+str(precision)+"\nRecall is :"+str(recall)+"\nF Score is :"+str(f_score))

# Average Bleu score for test set for 1000 samples
j=0
from nltk.translate.bleu_score import sentence_bleu
for i in range(1000):
  reference = d1.iloc[i, 0]
  candidate = a[i]
  score = sentence_bleu(reference, candidate)
  j=j+score
print(j/1000)

# Average Rouge-L score for test set 1000 samples

r = Rouge()
q=[]
i=0
while i<1000:
  q.append(r.rouge_l(a[i], d1.iloc[i, 0]))
  i=i+1
#print(p[0][0])

p=0
f=0
r=0
for i in range(1000):
  f=f+q[i][2]
  p=p+q[i][0]
  r=r+q[i][1]
p=p/1000
f=f/1000
r=r/1000
print("Average Precision is :"+str(p)+"\nAverage Recall is :"+str(r)+"\nAverage F Score is :"+str(f))

"""#**1. Comaparison with the paper's (Deep learning and semantic content generation) methodology**
# **2. Converted sentences to entities -> trained on transformer & comparing with our model result**
# **3. Ex: Trump has been in Chicago” -> (generalization) -> “_person_ has been in _location_**
"""

import spacy
nlp=spacy.load('en_core_web_sm')

nlp.pipe_names

a=[]
for i in range(10000):
  doc = nlp(document[i])
  newString = document[i]
  for e in reversed(doc.ents): #reversed to not modify the offsets of other entities when substituting
      start = e.start_char
      end = start + len(e.text)
      newString=newString[:start] + e.label_ + newString[end:]
  a.append(newString)
#print(newString)

port1 = pd.DataFrame({'sentence': a})
#port1

b=[]
for i in range(10000):
  doc = nlp(summary[i])
  newString = summary[i]
  for e in reversed(doc.ents): #reversed to not modify the offsets of other entities when substituting
      start = e.start_char
      end = start + len(e.text)
      newString=newString[:start] + e.label_ + newString[end:]
  b.append(newString)

port2 = pd.DataFrame({'summary': b})

d=port1['sentence']
s=port2['summary']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(d,s, test_size=0.30, random_state=42)

## Train and validation split
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.30, random_state=42)

"""### Preprocessing"""

d=X_train
s=y_train

# for decoder part
s = s.apply(lambda x: '<start> ' + x + ' <stop>')
s.head()

"""#### Tokenizing the texts into integer tokens"""

document_tokenizer.fit_on_texts(d)
summary_tokenizer.fit_on_texts(s)

inputs = document_tokenizer.texts_to_sequences(d)
targets = summary_tokenizer.texts_to_sequences(s)

summary_tokenizer.texts_to_sequences(["Testing statement here"])

encoder_vocab_size = len(document_tokenizer.word_index) + 1
decoder_vocab_size = len(summary_tokenizer.word_index) + 1

# vocab_size
encoder_vocab_size, decoder_vocab_size

#Epochs selected=20
# Here 5 epochs are starting from 1 as we are loading from previous checkpoints
# All above functions are already written at top and are called 

import random
random.seed(42)
for epoch in range(EPOCHS):
    start = time.time()

    train_loss.reset_states()
    val_loss.reset_states()
    for (batch, (inp, tar)) in enumerate(dataset):
        train_step(inp, tar)
        val_step(inp,tar)
      
    if (epoch + 1) % 5 == 0:
        ckpt_save_path = ckpt_manager.save()
        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))
    
    print ('Epoch {} train_Loss {:.4f}'.format(epoch + 1, train_loss.result()))
    print ('Epoch {} val_Loss {:.4f}'.format(epoch + 1, val_loss.result()))
    print ('Time taken for 1 epoch: {} secs\n'.format(time.time() - start))

"""Evaluation on validation set"""

k=pd.DataFrame(X_val)
#k['text'].head()
k.shape

# Taking 1000 entries as colab got crashed at higher values
b=[]
for i in range(1000):
  b.append(summarize(k.iloc[i, 0]))

d2=pd.DataFrame(y_val)

"""Summary example on validation set"""

print(k.iloc[280, 0]) #X_val

print(d2.iloc[280, 0]) #ground truth #y_val

b[280]  # predicted

print(k.iloc[465, 0]) #X_val

print(d2.iloc[465, 0]) #ground truth #y_val

b[465]  # predicted

"""# *Evaluation metric*"""

# Average Blue score for validation set

j=0
from nltk.translate.bleu_score import sentence_bleu
for i in range(1000):
  reference = d2.iloc[i, 0]
  candidate = b[i]
  score = sentence_bleu(reference, candidate)
  j=j+score
print(j/1000)

# Average Rouge-l score for validation set

r = Rouge()
m=[]
i=0
while i<1000:
  m.append(r.rouge_l(b[i],d2.iloc[i, 0]))
  i=i+1

p=0
r=0
f=0
for i in range(1000):
  f=f+q[i][2]
  p=p+q[i][0]
  r=r+q[i][1]
p=p/1000
f=f/1000
r=r/1000
print("Average Precision is :"+str(p)+"\nAverageRecall is :"+str(r)+"\nAverage F Score is :"+str(f))

"""# Evaluation on test set"""

d=pd.DataFrame(X_test)
#d['text'].head()
d.shape

# Taking 1000 test samples
a=[]
for i in range(1000):
  a.append(summarize(d.iloc[i, 0]))

d1=pd.DataFrame(y_test)

"""Example of summaries shown in report

A) Good summaries
"""

print(d.iloc[318,0]) #X_test

print(d1.iloc[318, 0]) #ground truth #y_test

a[318]  # predicted

print(d.iloc[199, 0]) #X_test

print(d1.iloc[199, 0]) #ground truth #y_test

a[199]  # predicted

"""B) Bad summaries"""

print(d.iloc[7, 0]) #X_test

print(d1.iloc[7, 0]) #ground truth #y_test

a[7]  # predicted

print(d.iloc[12, 0]) #X_test

print(d1.iloc[12, 0]) #ground truth #y_test

a[12]  # predicted

"""B) Bad summaries"""

print(d.iloc[7, 0]) #X_test

print(d1.iloc[7, 0]) #ground truth #y_test

a[7]  # predicted

print(d.iloc[12, 0]) #X_test

print(d1.iloc[12, 0]) #ground truth #y_test

a[12]  # predicted

"""Rouge-L score for good summaries"""

r = Rouge()
[precision, recall, f_score] = r.rouge_l(a[318], d1.iloc[318, 0])
print("Precision is :"+str(precision)+"\nRecall is :"+str(recall)+"\nF Score is :"+str(f_score))

r = Rouge()
[precision, recall, f_score] = r.rouge_l(a[199], d1.iloc[199,0])
print("Precision is :"+str(precision)+"\nRecall is :"+str(recall)+"\nF Score is :"+str(f_score))

"""Rouge-L score for bad summaries"""

r = Rouge()
[precision, recall, f_score] = r.rouge_l(a[7], d1.iloc[7, 0])
print("Precision is :"+str(precision)+"\nRecall is :"+str(recall)+"\nF Score is :"+str(f_score))

r = Rouge()
[precision, recall, f_score] = r.rouge_l(a[12], d1.iloc[12, 0])
print("Precision is :"+str(precision)+"\nRecall is :"+str(recall)+"\nF Score is :"+str(f_score))

# Average Bleu score for test set
j=0
from nltk.translate.bleu_score import sentence_bleu
for i in range(1000):
  reference = d1.iloc[i, 0]
  candidate = a[i]
  score = sentence_bleu(reference, candidate)
  j=j+score
print(j/1000)

# Average Rouge-L score for test set

r = Rouge()
q=[]
i=0
while i<1000:
  q.append(r.rouge_l(a[i], d1.iloc[i, 0]))
  i=i+1
#print(p[0][0])

p=0
f=0
r=0
for i in range(1000):
  f=f+q[i][2]
  p=p+q[i][0]
  r=r+q[i][1]
p=p/1000
f=f/1000
r=r/1000
print("Average Precision is :"+str(p)+"\nAverage Recall is :"+str(r)+"\nAverage F Score is :"+str(f))

"""## **BONUS question**"""

# This is processed data and pre-processing has been explained in report in our model

news = pd.read_csv("/content/drive/MyDrive/output-onlinerandomtools.csv")

news.head()

news.shape

document = news['Text']
summary = news['Summary']

# Epochs selected= 50

# All above functions are already written at top and are called here

import random
random.seed(42)
for epoch in range(EPOCHS):
    start = time.time()

    train_loss.reset_states()
    val_loss.reset_states()
    for (batch, (inp, tar)) in enumerate(dataset):
        train_step(inp, tar)
        val_step(inp,tar)
      
    if (epoch + 1) % 5 == 0:
        ckpt_save_path = ckpt_manager.save()
        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))
    
    print ('Epoch {} train_Loss {:.4f}'.format(epoch + 1, train_loss.result()))
    print ('Epoch {} val_Loss {:.4f}'.format(epoch + 1, val_loss.result()))
    print ('Time taken for 1 epoch: {} secs\n'.format(time.time() - start))

"""Evaluation on validation set"""

val_df.shape

# Taking 99 entries of validation set and storing in array
b=[]
for i in range(99):
  b.append(summarize(val_df.iloc[i, 3]))

"""Example of summaries shown in report

A) Good summary
"""

print(val_df.iloc[12, 3]) #X_val

print(val_df.iloc[12, 2]) #ground truth #y_val

b[12]  # predicted

# Rouge score

r = Rouge()
[precision, recall, f_score] = r.rouge_l(b[12], val_df.iloc[12,2])
print("Precision is :"+str(precision)+"\nRecall is :"+str(recall)+"\nF Score is :"+str(f_score))

"""B) Bad summary"""

print(val_df.iloc[13, 3]) #X_val

print(val_df.iloc[13, 2]) #ground truth #y_val

b[13]  # predicted

# Rouge score

r = Rouge()
[precision, recall, f_score] = r.rouge_l(b[13], val_df.iloc[13,2])
print("Precision is :"+str(precision)+"\nRecall is :"+str(recall)+"\nF Score is :"+str(f_score))

"""Evaluation metric"""

j=0
from nltk.translate.bleu_score import sentence_bleu
for j in range(5):
  for i in range(0,99,20):
    reference = val_df.iloc[i, 2]
    candidate = b[i]
    score = sentence_bleu(reference, candidate)
    j=j+score
  print('blue score for every 20 samples',j/20)
  print("\n")

# Average Blue score for validation set

j=0
from nltk.translate.bleu_score import sentence_bleu
for i in range(99):
  reference = val_df.iloc[i, 2]
  candidate = b[i]
  score = sentence_bleu(reference, candidate)
  j=j+score
print(j/99)

#  Rouge-L score for validation set

r = Rouge()
m=[]
i=0
while i<99:
  m.append(r.rouge_l(b[i],val_df.iloc[i, 2]))
  i=i+1

p=0
r=0
f=0
for j in range(5):
  for i in range(0,99,20):
    f=f+m[i][2]
    p=p+m[i][0]
    r=r+m[i][1]
  p=p/20
  f=f/20
  r=r/20
  print('for every 20 samples:')
  print(" Average Precision is :"+str(p)+"\nAverageRecall is :"+str(r)+"\nAverage F Score is :"+str(f))
  print('\n')

# Average Rouge-L score for validation set

p=0
r=0
f=0
for i in range(99):
  f=f+m[i][2]
  p=p+m[i][0]
  r=r+m[i][1]
p=p/99
f=f/99
r=r/99
print("Average Precision is :"+str(p)+"\nAverageRecall is :"+str(r)+"\nAverage F Score is :"+str(f))